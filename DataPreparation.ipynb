{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code for the preparation of the data for the datasets from the British Lexicon Project (BLP) and the Calgary Semantic Decision Project (CSDP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the libraries required for data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "suppressMessages(library(vwr))\n",
    "suppressMessages(library(ndl))\n",
    "library(MASS)\n",
    "library(readxl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## British Lexicon Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation of the BLP data. We read in the response time data, take the relevant subset from them, and compute the lexical-distributional variables for the response time analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "blp = read.table(\"input/blp-items.txt\", header = TRUE)\n",
    "\n",
    "# Restrict to relevant columns\n",
    "blp = blp[,c(\"spelling\",\"rt\", \"accuracy\")]\n",
    "colnames(blp) = c(\"Word\", \"RT\", \"Accuracy\")\n",
    "\n",
    "# Remove words without response time\n",
    "blp = blp[which(!is.na(blp$RT)),]\n",
    "\n",
    "# Limit to words with a minimum accuracy of 0.75 in the BLP\n",
    "blp = blp[which(blp$Accuracy >= 0.75),]\n",
    "\n",
    "# Read in word frequencies\n",
    "frequencies = read.table(\"input/blp-stimuli.txt\", sep = \"\\t\", header = TRUE)\n",
    "frequencies = frequencies[,c(\"spelling\", \"subtlex.frequency\")]\n",
    "colnames(frequencies) = c(\"Word\", \"Frequency\")\n",
    "\n",
    "# Combine \n",
    "blp = merge(blp, frequencies, by = \"Word\")\n",
    "\n",
    "# Limit to words with a minimum frequency of 10 in SUBTLEX-UK\n",
    "blp = blp[which(blp$Frequency >= 10),]\n",
    "\n",
    "# Define word length\n",
    "blp$Length = nchar(blp$Word)\n",
    "\n",
    "# Define OLD20\n",
    "blp$OLD20 = as.numeric(old20(blp$Word,blp$Word))\n",
    "blp$OLD20Norm = blp$OLD20 / blp$Length\n",
    "\n",
    "# Define mean bigram frequency\n",
    "bigram_list = strsplit(orthoCoding(blp$Word), \"_\")\n",
    "bigrams = unique(unlist(bigram_list))\n",
    "bigram_freqs = sapply(bigrams, FUN = function(x) {\n",
    "  sum(blp$Frequency[which(unlist(lapply(bigram_list, function(y){x%in%y})))])  \n",
    "})\n",
    "blp$MeanBigramFrequency = unlist(lapply(bigram_list, \n",
    "  function(x){sum(bigram_freqs[x])/length(x)}))\n",
    "\n",
    "# Run Box-Cox tests to determine appropriate transformations\n",
    "# par(mfrow=c(2,3))\n",
    "# for(pred in c(\"RT\", \"Frequency\", \"Length\", \"OLD20Norm\", \"MeanBigramFrequency\")) {\n",
    "#   boxcox(blp[,pred] ~ 1)\n",
    "# }\n",
    "\n",
    "# Apply tranformations\n",
    "blp$RTInv = -1000 / blp$RT\n",
    "blp$LogFrequency = log(blp$Frequency + 1)\n",
    "blp$LogOLD20Norm = log(blp$OLD20Norm)\n",
    "blp$LogMeanBigramFrequency = log(blp$MeanBigramFrequency + 1)\n",
    "\n",
    "# Restrict to relevant columns\n",
    "blp = blp[,c(\"Word\", \"RT\", \"RTInv\", \"Frequency\", \"LogFrequency\", \"Length\", \n",
    "  \"LogOLD20Norm\", \"LogMeanBigramFrequency\")]\n",
    "\n",
    "# Rename rows\n",
    "rownames(blp) = 1:nrow(blp)\n",
    "\n",
    "# Save data\n",
    "saveRDS(blp, file = \"data/blp.rds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calgary Semantic Decision Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation of the CSDP data. We read in the response time data, take the relevant subset from them, and compute the lexical-distributional variables for the response time analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "csdp = read_excel(\"input/CSD.xlsx\")\n",
    "\n",
    "# Restrict to relevant columns\n",
    "csdp = csdp[,c(\"Word\",\"RTclean_mean\", \"WordType\", \"Block\")]\n",
    "colnames(csdp) = c(\"Word\", \"RT\", \"Type\", \"Block\")\n",
    "\n",
    "# Remove words without response time\n",
    "csdp = csdp[which(!is.na(csdp$RT)),]\n",
    "\n",
    "# Get lexical-distributional variables from the blp data frame\n",
    "blp_var = blp[,c(\"Word\", \"Frequency\", \"LogFrequency\", \"Length\", \"LogOLD20Norm\", \n",
    "  \"LogMeanBigramFrequency\")]\n",
    "csdp = merge(csdp, blp_var, by = \"Word\")\n",
    "\n",
    "# Limit to words with a minimum frequency of 10 in SUBTLEX-US\n",
    "csdp = csdp[which(csdp$LogFrequency >= log(10)),]\n",
    "\n",
    "# Run Box-Cox test to determine appropriate transformation for response times\n",
    "# boxcox(csdp$RT ~ 1)\n",
    "\n",
    "# Transform response times\n",
    "csdp$LogRT = log(csdp$RT)\n",
    "\n",
    "# Rename rows\n",
    "rownames(csdp) = 1:nrow(csdp)\n",
    "\n",
    "# Save data\n",
    "saveRDS(csdp, file = \"data/csdp.rds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
